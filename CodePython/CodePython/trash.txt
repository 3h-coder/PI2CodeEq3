if(("next" in a.text) or ("next" in a['href'])):
            nextURL=a['href']
        elif(("Next" in a.text) or ("Next" in a['href'])):
            nextURL=a['href']
        elif(("older" in a.text) or ("older" in a['href'])):
            nextURL=a['href']
        elif(("Older" in a.text) or ("Older" in a['href'])):
            nextURL=a['href']

elif("Next Page" in str(a)): 
                newpage=requests.get(a.get('href'))
                webbrowser.open(a.get('href'))

"https://www.darkreading.com/attacks-breaches",

            else:
                nextpageURL=FindNextPage(soup)
                if(nextpageURL!=""):
                    requests.get(nextpageURL)
                    webbrowser.open(nextpageURL)
                else:
                    pass


for a in anchors:
            if (company in a.get('href')) or (company in a.text):
                newpage=requests.get(a.get('href'))
                newsoup=BeautifulSoup(newpage.text, "lxml")
                for paragraph in newsoup.find_all('p'):
                    print(paragraph.text)
                found=True
                #webbrowser.open(a.get('href'))
        counter=0
        while(found==False or counter<5): # Si l'on a rien trouvé sur cette page, on scrape la suivante ou on continue à scroll down (jusqu'à quand?)
            nextpageURL=FindNextPage(soup)
            if(nextpageURL!=""):
                nextpage=requests.get(nextpageURL)
                soup=BeautifulSoup(nextpage.text, "lxml")
                anchors=soup.find_all('a')
                for a in anchors:
                    if (company in a.get('href')) or (company in a.text):
                        newpage=requests.get(a.get('href'))
                        newsoup=BeautifulSoup(newpage.text, "lxml")
                        for paragraph in newsoup.find_all('p'):
                            print(paragraph.text)
                        found=True
                        webbrowser.open(nextpageURL)
            else:
                counter=counter+1



def FindNextPage(soup): #Code général pour trouver le lien de la prochaine page (difficile pour plusieurs sources)
    nextURL=""
    anchors=soup.find_all('a')
    for anchor in anchors:
        a=str(anchor)
        if(("Next" in a) or ("next" in a) or ("Page" in a) or ("page" in a) or \
          ("Older" in a) or ("older" in a) ):
            if(anchor['href'].startswith("https://")):
                nextURL=anchor['href']
    return nextURL



def webscraping(company): #/!\ La recherche est Case sensitive (les majuscules/minuscules sont différenciées)
    #(Peut-être) Faire un pattern tel que: 
    #1 Dans un premier temps on regarde si l'entreprise apparait dans la page
    #2 Si elle apparait dans un bloc qui mène à un lien, ouvrir le lien pour scraper la nouvelle page
    #Le but est de créer un programme général de manière à pouvoir ajouter les sources au fur et à mesure.

    sources=["https://thehackernews.com/"] #On part du principe que nos sources sont fiables ici  (les ajouter et tester au fur et à mesure)

    for source in sources:
        #print(source+"\n")
        found=False
        counter=0
        mainpage=requests.get(source)
        soup=BeautifulSoup(mainpage.text, "lxml")
        anchors=soup.find_all('a')
        for a in anchors:
            if (company in a.get('href')) or (company in a.text):
                newpage=requests.get(a.get('href'))
                newsoup=BeautifulSoup(newpage.text, "lxml")
                for paragraph in newsoup.find_all('p'):
                    print(paragraph.text)
                found=True
                #webbrowser.open(a.get('href'))
        while(found==False and counter<5): #Scraper la page suivante tant que l'on a pas trouvé ou scraper moins de 5 pages
            nextpageURL=FindNextPage(soup)
            if(nextpageURL!=""): #On scrape la page suivante
                counter=counter+1
                nextpage=requests.get(nextpageURL)
                webbrowser.open(nextpageURL)
                soup=BeautifulSoup(nextpage.text, "lxml")
                anchors=soup.find_all('a')
                for a in anchors:
                    if (company in a.get('href')) or (company in a.text):
                        newpage=requests.get(a.get('href'))
                        newsoup=BeautifulSoup(newpage.text, "lxml")
                        for paragraph in newsoup.find_all('p'):
                            print(paragraph.text)
                        webbrowser.open(a.get('href'))
                        found=True
            else: #pas de page suivante
                pass