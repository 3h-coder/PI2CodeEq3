mainpage=requests.get("https://twitter.com/"+username)
        #On vérifie si les noms utilisateurs sont (toujours) valides
        soup=BeautifulSoup(mainpage.text,"lxml")
        for span in soup.find_all('span'):
            print(span)
            if(span.text=="This account doesn’t exist"): #Le compte n'existe pas
                print("It seems like https://twitter.com/"+username+" does not exist, please check the username.")
                break
            else:
                print("Nothing for now.")




#Recherche sur twitter des mots clés et renvoie un tableau des tweets les plus récents (au max 15 tweets)
def RechercheTweetsRecents(query):
    client = getClient()
    searchResults = client.search_recent_tweets(query=query, max_results = 15)

    tweets = searchResults.data

    results=[]

    if tweets is not None and len(tweets) != 0:
        for tweet in tweets:
            temp = {}
            temp['id'] = tweet.id
            temp['text'] = tweet.text
            results.append(temp) #results est un tableau de dictionnaires
    
    return results

#Test de la fonction RechercheTweetsRecents
#tweets = RechercheTweetsRecents("cyberattaque")
#for tweet in tweets:
#    print(tweet)


#Recherche parmi les 10 derniers tweets de l'utilisateur s'il a mentionné l'entreprise recherchée
def SearchTweetsUser2(username, company):
    id = getUserId(username)
    mention = False #Si l'on a trouvé un tweet à propos de l'entreprise

    url = 'https://api.twitter.com/2/users/{}/tweets'.format(id)
    #le bearer_token permet de se connecter à l'API de twitter
    with open("keys.txt", "r") as secretfile:
        bearer_token=secretfile.readline().rstrip()
    secretfile.close()
    headers = {'Authorization': 'Bearer {}'.format(bearer_token)}
    ListAlarmingTweets = []
    response = requests.request('GET', url, headers = headers) #mettre new_url
    tweetsData = response.json()
    for tweetData in tweetsData['data']:
        if(company in tweetData['text']):
            ListAlarmingTweets.append(tweetData)
            mention = True
            #print(tweetData)
                
    return mention, ListAlarmingTweets #On retourne ici un tuple



# On commence par l'exemple de SolarWinds, grande entreprise de contrôle de systèmes informatiques, victime d'une Cyberattaque de grande ampleur en 2020.
# A priori nous aurons déjà nos sources prédéfinies et lorsque que nous nous intéresserons au statut d'une entreprise en particulier,
# nous parcourrons nos sources à l'aide de mots clé (dont le nom de l'entreprise).
# Cependant ici à titre de découverte du web scraping l'approche est un peu différente, nous automatisons le processus de recherche qu'une personne lamba ferait sur Google.
def introwebscraping():
    query= "SolarWinds Cyberattaque" #La recherche que l'on effectue sur Google
    links =[] #Liste qui contiendra tous les liens des sites webs que nous allons "scraper" à l'issue de la recherche.

    for j in search(query, num=3, stop=3, pause=0.5): #On se contente des 3 résultats jugés les plus pertinents par Google dans cet exemple, idéalement en prendre le plus possible.
    #print(j)
        webbrowser.open(j)
        links.append(j) #On sauvegarde les liens

    for link in links:
        print("-------------------------------------------------------------------------------------------------------------------------------------------------------")
        page=requests.get(link)
        soup=BeautifulSoup(page.text, "lxml")  #Lecture du code source de la page
        print(link+"\n")
        print(soup.find("title").text+"\n") #Titre de la page (à priori un article)
        paragraphs=soup.find_all("p")
        keywords=["cyberattaque"] #mots clé pour nous aider à extraire l'information voulue
        keysentences=[] #phrases clé qu'il faudra analyser 
        for paragraph in paragraphs: #Parcourir les paragraphes pour en extraire les informations relatives à une attaque ou faille de sécurité.
            c=nlp(paragraph.text) #Conversion du texte en un objet spacy
            sentences=list(c.sents) 
            for sentence in sentences:
                for keyword in keywords:
                    if keyword in str(sentence):
                        print(sentence)
                        keysentences.append(sentence)


def SoupTest():
    URL="https://thehackernews.com/"
    headers={"User-Agent" : "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.81 Safari/537.36"}
    mainpage=requests.get(URL, headers=headers)
    soup=BeautifulSoup(mainpage.text, "lxml") #On scrape la première page
    print(soup)
    anchors=soup.find_all('a', {"class": "story-link"})
    for a in anchors:
        titles=a.find_all('h2')
        for title in titles:
            print(title.text)


    def RunAnalysis(self):
        """
        Analyzes the text variable to update the status, result, date and crit_sents variables.
        """
        self.date=datetime.now()
        self.result="Nothing to report."
        compsentences=TextAnalysis.DetectSentences(self.text, [self.company]) 
        for sentence_index in compsentences:
            #print(str(compsentences[sentence_index]))
            subj=TextAnalysis.IdentifySubject(str(compsentences[sentence_index]))
            #print("Subject: "+str(subj))
            for kw in TextAnalysis.keywords:
                if kw in str(compsentences[sentence_index]):
                    self.status=1
                    self.result="/!\\ Alert raised. /!\\"
                    critical_sentence=str(compsentences[sentence_index])
                    self.crit_sents.append(critical_sentence)